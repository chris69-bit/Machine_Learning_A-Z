# -*- coding: utf-8 -*-
"""Simple_House_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QMUG8uDoSZC40DWp2zURhLEhEl0S4yRX

#Import Libraries
"""

!pip install -q lazypredict
!pip install -q optuna
!pip install -q catboost

import pandas as pd
import numpy as np
import missingno
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', None)

from sklearn.preprocessing import PowerTransformer as powerTransformer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import Ridge, Lasso, LinearRegression
from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, ExtraTreesRegressor

from lazypredict.Supervised import LazyRegressor
import optuna
import xgboost as xgb
import lightgbm
import catboost

"""#Importing Dataset"""

test_dataset = pd.read_csv('/content/test.csv')
train_dataset = pd.read_csv('/content/train.csv')


combine_dataset = pd.concat([train_dataset, test_dataset], sort=False)

combine_dataset.shape

"""#Exploratory Data Analysis

##Checking Info
"""

combine_dataset.info()

"""##Checking Null Values"""

combine_null = combine_dataset.isnull().sum()

combine_null.drop(labels='SalePrice', axis=0, inplace=True)

#Convert to DataFrame
missing_values = pd.DataFrame(data=combine_dataset.isna().sum(), index=combine_dataset.columns, columns=['missing'])

#Add Percentage Column
missing_values['Percentage'] = missing_values['missing'] / 2919 * 100

#Sort By Descending Order
missing_values = missing_values.sort_values(by='Percentage', ascending=False)

missing_values.head(10)

"""##Visualize Missing Values"""

#Visualizing Missing Values With HeatMaps
sns.heatmap(combine_dataset.isnull(), cbar=False)

"""##Checking Data Distribution"""

#First Let's Split the data into numerical, categorical

numerical_data = combine_dataset.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_data = combine_dataset.select_dtypes(include='object').columns.tolist()

#Drop Id and SalesPrice
numerical_data = [col for col in numerical_data if col not in ['Id', 'SalePrice']]

#Further Split Numerical data into discrete and continuous data
discrete_data = [col for col in numerical_data if len(combine_dataset[col].unique()) > 25]
continuous_data = [col for col in numerical_data if col not in discrete_data]

print(f'Number of Categorical Feature : {len(categorical_data)}')
print(f'Number of Numerical Feature   : {len(numerical_data)}')
print(f'Number of Discrete Feature    : {len(discrete_data)}')
print(f'Number of Continous Feature   : {len(continuous_data)}')

#Visualize Distribution Of Categorical Feature
fig , axes = plt.subplots(nrows=9, ncols=5, figsize=(30,30))

for i, feature in enumerate(categorical_data):
  sns.histplot(x=feature, data=train_dataset, ax=axes[i//5, i%5])
  sns.histplot(x=feature, data=test_dataset, ax=axes[i//5, i%5])

#Visualize Distribuion of Discrete data
fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 10))

for i, feature in enumerate(discrete_data):
  sns.histplot(x=feature, data=train_dataset, ax=axes[i//5, i%5])
  sns.histplot(x=feature, data=test_dataset, ax=axes[i//5, i%5])
plt.show()

#Visualize Distribution of Continuous data
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 15))

for i, feature in enumerate(continuous_data):
  sns.histplot(x=feature, data=train_dataset, ax=axes[i//5, i%5])
  sns.histplot(x=feature, data=test_dataset, ax=axes[i//5, i%5])

"""##Checking Skewness"""

skewness_train = train_dataset[numerical_data].skew().sort_values(ascending=False)
skewness_test = test_dataset[numerical_data].skew().sort_values(ascending=False)

avg_skewness = (skewness_train + skewness_test) / 2
avg_skewness = avg_skewness.sort_values(ascending=False)

print(avg_skewness)

"""###Skew values with more than 1 or -1 will need transformation"""

sns.histplot(x='SalePrice', data=train_dataset)

"""##Correlation Comparison"""

#Check Correlation between each feature
numerical_feature = pd.DataFrame(data=train_dataset[numerical_data])

corr_data = numerical_feature.corr(method='pearson')
plt.figure(figsize=(30,30))
sns.heatmap(data= corr_data, cmap='coolwarm', annot=True, fmt='.2g')

"""There is a strong positive correlation between

    1. GarageArea and GarageCars.

    2. 1stFlrSF and TotalBsmtSF.

    3. GrLivArea and TotRmsAbvGrd.

we can consider removing one of them or using PCA to select the most important features
"""

numerical_feature['SalePrice'] = train_dataset['SalePrice']

corr_data = numerical_feature.corr(method='pearson')
corr_data = corr_data[['SalePrice']]

plt.figure(figsize=(30,30))
sns.heatmap(data= corr_data, cmap='coolwarm', annot=True, fmt='.2g')

# LINEARITY USING SCATTER PLOT

fig , axes = plt.subplots(nrows=7, ncols=6, figsize=(37,25))

for i , feature in enumerate(numerical_data):
    sns.regplot(data= train_dataset, x= feature, y= 'SalePrice', ax= axes[i%7, i//7])

plt.show()

"""#Feature Engineering

##Drop Columns
"""

#Drop Columns With Many Null Values
cols_with_many_nulls = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu', 'LotFrontage']
combine_dataset = combine_dataset.drop(labels=cols_with_many_nulls, axis=1)

# NEXT DROP COLUMNS WITH MANY ZERO VALUES
cols_with_many_zero = ['LowQualFinSF', 'MiscVal', '3SsnPorch', 'PoolArea']
combine_dataset = combine_dataset.drop(labels=cols_with_many_zero, axis=1)

# LAST DROP COLUMNS WITH DOMINANT 1 LABEL
cols_with_dominant_label = ['Id','RoofMatl', 'Street', 'Condition2', 'Utilities', 'Heating']
combine_dataset = combine_dataset.drop(labels=cols_with_dominant_label, axis=1)

combine_dataset.info()

"""##Temporal Variable"""

# FIND ALL DATE FEATURE
year_feature = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']

for feature in year_feature:
    combine_dataset[feature] = combine_dataset['YrSold'] - combine_dataset[feature]

combine_dataset[year_feature].head(5)

"""##Filling Missing Values"""

# Step 1: Check which ones have missing values
missing_categoricals = [col for col in categorical_data if train_dataset[col].isnull().sum() > 0]

# Step 2: Check mode for each
for col in missing_categoricals:
    mode_val = train_dataset[col].mode()[0]
    print(f"Feature: {col} | Missing: {train_dataset[col].isnull().sum()} | Mode: {mode_val}")

#Fill missing values with zeros
for feature in numerical_data:
  if feature not in cols_with_many_nulls and feature not in cols_with_many_zero:
    combine_dataset[feature] = combine_dataset[feature].fillna(0)

#Dealing with Categorical Features
dropped_cols = cols_with_dominant_label + cols_with_many_nulls
mode_features = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']

for feature in categorical_data:
  if feature not in dropped_cols:
    if feature not in mode_features:
      combine_dataset[feature] = combine_dataset[feature].fillna('Unknown')
    else:
      combine_dataset[feature] = combine_dataset[feature].fillna(combine_dataset[feature].mode()[0])

combine_dataset.describe()

"""##Combining Variables and Creating New Features"""

#Add New Feature 'Garage Efficiency'
combine_dataset['GarageEfficiency'] = combine_dataset['GarageArea'] / (combine_dataset['GarageCars'] +1)

# ADD NEW FEATURE TotalArea
combine_dataset['TotalArea'] = combine_dataset['GrLivArea'] + combine_dataset['TotalBsmtSF']

# DROP MULTICOLINEARITY FEATURE

multi_corr = ['GarageCars','GarageArea', 'TotalBsmtSF','GrLivArea', 'TotRmsAbvGrd']

combine_dataset = combine_dataset.drop(labels= multi_corr, axis=1)

combine_dataset.head(5)

combine_dataset.info()

"""#Feature Transformation"""

#Let check the distribution of the features one more time


numerical_data_2 = [col for col in combine_dataset.select_dtypes(include=['int64', 'float64']).columns.tolist()]

#Further Split Numerical data into discrete and continuous data
discrete_data_2 = [col for col in numerical_data_2 if len(combine_dataset[col].unique()) > 25]
continuous_data_2 = [col for col in numerical_data_2 if col not in discrete_data_2]

print(f'Number of Numerical Feature   : {len(numerical_data_2)}')
print(f'Number of Discrete Feature    : {len(discrete_data_2)}')
print(f'Number of Continous Feature   : {len(continuous_data_2)}')

def get_outlier_counts_iqr(df):
    outlier_counts = {}
    for col in df.select_dtypes(include='number'):
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        count = ((df[col] < lower) | (df[col] > upper)).sum()
        outlier_counts[col] = count
    return pd.Series(outlier_counts).sort_values(ascending=False)

outlier_summary = get_outlier_counts_iqr(combine_dataset)
print(outlier_summary)

def cap_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[column] = np.clip(df[column], lower, upper)
    return df

for col in ['EnclosedPorch', 'BsmtFinSF2', 'LotArea', 'ScreenPorch', 'OverallCond']:
    combine_dataset = cap_outliers_iqr(combine_dataset, col)

fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(15, 15))

for i, feature in enumerate(discrete_data_2):
  sns.histplot(x=feature, data=combine_dataset, ax=axes[i//5, i%5])
plt.show()

skewness = combine_dataset[numerical_data_2].skew().sort_values(ascending=False)


print(skewness)

#Columns to Transform have values greater than 1 or -1
cols_to_transform = ['LotArea', 'KitchenAbvGr', 'BsmtFinSF2', 'EnclosedPorch', 'ScreenPorch', 'BsmtHalfBath', 'MasVnrArea', 'OpenPorchSF','WoodDeckSF', 'TotalArea', '1stFlrSF', 'BsmtFinSF1', 'MSSubClass']

for col in cols_to_transform:
  transformer = powerTransformer(method='yeo-johnson', standardize=True, copy=True)
  combine_dataset[[col]] = transformer.fit_transform(combine_dataset[[col]])

skewness = combine_dataset[numerical_data_2].skew().sort_values(ascending=False)

print(skewness)

"""##Categorical Data Tranformation

###One Hot Encoder
"""

combine_dataset = pd.get_dummies(combine_dataset).reset_index(drop=True)
combine_dataset.info()

"""##Re-Split The Dataset"""

new_train_dataset = combine_dataset.iloc[:len(train_dataset), :]
new_test_dataset = combine_dataset.iloc[len(train_dataset):, :]

x_train = new_train_dataset.drop(labels='SalePrice', axis=1)

y_train = np.log1p(new_train_dataset['SalePrice'])

x_test = new_test_dataset.drop(labels=['SalePrice'], axis=1)

x_train.shape , y_train.shape, x_test.shape

"""##Feature Scaling"""

cols_to_robust = [
    'BsmtHalfBath', 'KitchenAbvGr', 'GarageEfficiency',
    'BsmtUnfSF', '2ndFlrSF', 'Fireplaces', 'HalfBath', 'BsmtFullBath',
    'YearBuilt', 'OverallCond'
]

cols_to_zscore = [
    'GarageYrBlt', 'YearRemodAdd', 'MasVnrArea', 'BedroomAbvGr', 'OverallQual',
    'MoSold', 'FullBath', 'YrSold', 'WoodDeckSF', 'MSSubClass', 'LotArea', 'TotalArea',
    'OpenPorchSF', 'ScreenPorch', 'BsmtFinSF2', 'EnclosedPorch', '1stFlrSF',
    'BsmtFinSF1'
]

#Robust Scaling
robust_scaler = RobustScaler()
robust_scaler.fit(x_train[cols_to_robust])

x_train[cols_to_robust] = robust_scaler.transform(x_train[cols_to_robust])
x_test[cols_to_robust]  = robust_scaler.transform(x_test[cols_to_robust])

# ZSCORE NORM
zscore = StandardScaler()
zscore.fit(x_train[cols_to_zscore])

x_train[cols_to_zscore] = zscore.transform(x_train[cols_to_zscore])
x_test[cols_to_zscore]  = zscore.transform(x_test[cols_to_zscore])

x_train.shape , x_test.shape

"""#Model Development

##Lazy Predict
"""

#Split the x_train and y_train

x_train_lazy , x_test_lazy , y_train_lazy , y_test_lazy = train_test_split(x_train, y_train, test_size=0.2, random_state=12, shuffle=True)

lazy_predict = LazyRegressor(verbose=0, random_state=12, regressors='all')

train_lazy, test_lazy = lazy_predict.fit(x_train_lazy, x_test_lazy, y_train_lazy, y_test_lazy)
test_lazy

"""we can see above the best algorithm. Ridge and Linear Regression are better compare to each other. we can use them for Stacking-Models.

so, i want use Linear Model + Non-Linear Model Combination for Stacking Model .

Here my Combination :

Ridge Regression (Linear Model)

Linear Regression (Linear Model)

Lasso Regression (Linear Model)

Gradient Boosting Machine (Non-Linear Model)

XGBoost (Non-Linear Model)

LightGBM (Non-Linear Model)

CatBoost (Non-Linear Model)

##Use Optuna For HyperParameters

###Ridge Regression
"""

#Find best Alpha and Max_tier for Ridge
def Ridge_objective(trial):
  alpha = trial.suggest_float('alpha',0.1, 25)
  max_iter = trial.suggest_int('max_iter',150,2000)

  #Set Model
  model = Ridge(alpha=alpha, max_iter=max_iter,random_state=12)

  #Score Metrics
  score = cross_val_score(estimator=model, X=x_train, y=y_train, scoring='neg_root_mean_squared_error')

  return score.mean()

# BUILD A OPTUNA
study = optuna.create_study(direction='maximize')
study.optimize(func= Ridge_objective, n_trials=100)

print(f'Best CV    : {study.best_params}')
print(f'Best Score : {study.best_value}')

#GET BEST HYPERPARAMETERS
ridge_best_params = study.best_params

#Build The Model
ridge = Ridge(alpha=ridge_best_params['alpha'], max_iter=ridge_best_params['max_iter'], random_state=12, solver='auto', fit_intercept=True)

#Fit The Model
ridge.fit(x_train, y_train)

"""##Feature Importance Of Ridge"""

# This is to see useful features and useless features for Ridge
# useful_features = {} #Capture coefficients of high value
# useless_features = {} #Capure coefficients with values close to zero

"""###Check Feature Importance"""

# coefficients = ridge.coef_
# features_importance = pd.DataFrame({
#     'feature':ridge.feature_names_in_,
#     'coefficient': coefficients
# })

# features_importance = features_importance.sort_values(by='coefficient', ascending=False)

# plt.figure(figsize=(15,15))
# plt.barh(y=features_importance['feature'], width=features_importance['coefficient'])
# plt.show()

"""Lets display features with coefficients close to zero"""

# features_importance['coefficient'] = abs(features_importance['coefficient']) #First convert coefficients to positive value
# zero = features_importance[features_importance['coefficient'] < 0.001].sort_values(by='feature', ascending=True)

# #Put the Features in a Dictotionary
# for feature in zero['feature']:
#   useless_features[feature] = useless_features.get(feature, 0) +1

# #Put The High Coefficient into Dict
# threshold = 0.02
# high_coef = features_importance[features_importance['coefficient'] >= threshold].sort_values(by='coefficient', ascending=False) # SORT IT DESCENDING
# high_coef = high_coef.reset_index(drop=True)

# for i, feature in enumerate(high_coef['feature']):
#     useful_features[feature] = useful_features.get(feature, 0) + high_coef['coefficient'][i] # COEFFICIENTS WITH HIGHER WEIGHT WILL GET MORE POINTS/VALUES

# # DISPLAY FEATURE THAT HAS VALUE CLOSE TO ZERO
# zero

"""##Lasso Regression"""

def lasso_objective(trial):
    # SET HYPERPARAMETERS VALUE RANGE
    alpha    = trial.suggest_float('alpha',0.1, 25)
    max_iter = trial.suggest_int('max_iter',150, 2000)

    # SET MODEL
    model = Lasso(alpha=alpha,
                  max_iter=max_iter,
                  random_state=10)

    # SCORE METRICS
    score = cross_val_score(estimator= model , X = x_train, y= y_train, scoring='neg_root_mean_squared_error')

    return score.mean()

study = optuna.create_study(direction='maximize')
study.optimize(func=lasso_objective, n_trials=100)

print(f'Best CV: {study.best_params}')
print(f'Best Score: {study.best_value}')

lasso_best_params = study.best_params

lasso = Lasso(alpha=lasso_best_params['alpha'], max_iter=lasso_best_params['max_iter'], random_state=12, fit_intercept=True)

lasso.fit(x_train, y_train)

"""##Linear Regression"""

linear = LinearRegression(fit_intercept=True, positive=False)

linear.fit(x_train, y_train)

"""##Gradient Boosting Machine"""

def gbm_objective(trial):
  n_estimators = trial.suggest_int('n_esitimators', 50,2000)
  learning_rate = trial.suggest_float('learning', 0.001,1)
  max_depth = trial.suggest_int('max_depth', 2,16)
  max_features = trial.suggest_int('max_features', 5, 60)
  min_samples_split = trial.suggest_int('min_samples_split', 2, 25)

    # DECLARATE THE MODEL AND ITS HYPERPARAMETERS
  gbr = GradientBoostingRegressor(
      n_estimators      = n_estimators,
      learning_rate     = learning_rate,
      max_depth         = max_depth,
      min_samples_split = min_samples_split,
      max_features      = max_features,
      random_state      = 12
        )
  score = cross_val_score(estimator=gbr, X=x_train, y=y_train, scoring='neg_root_mean_squared_error')

  return score.mean()

#Hyperparameter Tuning with Optuna
study = optuna.create_study(direction='maximize')
study.optimize(func=gbm_objective, n_trials=100)

print(f'Best Hyperparameter : {study.best_params}')
print(f'Best Score          : {study.best_value}')

#TAKE THE BEST PARAMETERS
gbm_best_params = study.best_params

gbr = GradientBoostingRegressor(
    n_estimators=gbm_best_params['n_esitimators'],
    learning_rate=gbm_best_params['learning'],
    max_depth=gbm_best_params['max_depth'],
    min_samples_split=gbm_best_params['min_samples_split'],
    max_features=gbm_best_params['max_features'],
    random_state=12
)
gbr.fit(x_train, y_train)

"""#XGBoost"""

def xgb_objective(trial):
  n_estimators = trial.suggest_int('n_estimators',100, 2000)
  learning_rate = trial.suggest_float('learning_rate', 0.01, 1)
  max_depth = trial.suggest_int('max_depth',2,20)
  min_child_weight = trial.suggest_float('min_child_weight', 0.4, 10)
  subsample = trial.suggest_float('subsample',0.35,1)
  gamma = trial.suggest_float('gamma', 0.1,1)
  reg_alpha = trial.suggest_float('reg_alpha', 0.01,15)
  reg_lambda = trial.suggest_float('reg_lambda', 0.01,15)

  xgb_model = xgb.XGBRegressor(
      n_estimators=n_estimators,
      learning_rate=learning_rate,
      max_depth=max_depth,
      min_child_weight=min_child_weight,
      subsample=subsample,
      gamma=gamma,
      reg_alpha=reg_alpha,
      reg_lambda=reg_lambda,
      random_state=12
  )

  score = cross_val_score(estimator=xgb_model, X=x_train, y=y_train, scoring='neg_root_mean_squared_error')

  return score.mean()

study = optuna.create_study(direction='maximize')
study.optimize(func=xgb_objective, n_trials=100)

print(f'Best Hyperparameter : {study.best_params}')
print(f'Best Score          : {study.best_value}')

xgb_best_params = study.best_params

xgboost = xgb.XGBRegressor(
    n_estimators = xgb_best_params['n_estimators'],
    learning_rate = xgb_best_params['learning_rate'],
    max_depth = xgb_best_params['max_depth'],
    min_child_weight = xgb_best_params['min_child_weight'],
    subsample = xgb_best_params['subsample'],
    gamma = xgb_best_params['gamma'],
    reg_alpha = xgb_best_params['reg_alpha'],
    reg_lambda = xgb_best_params['reg_lambda'],
    random_state = 12
)

xgboost.fit(x_train, y_train)

"""##CatBoost"""

def catboost_objective(trial):
  params = {
    'iterations': trial.suggest_int('iterations', 100, 1000),
    'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 0.1),
    'depth': trial.suggest_int('depth', 3, 10),
    'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-5, 10),
    'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
    'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.5, 1.0),
    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),
    'max_bin': trial.suggest_int('max_bin', 10, 500),
    'eval_metric': 'RMSE',

    }

  cat = catboost.CatBoostRegressor(**params, random_state=12, verbose=0)

  score = cross_val_score(estimator=cat, X= x_train, y= y_train, scoring='neg_root_mean_squared_error')

  return score.mean()

study = optuna.create_study(direction='maximize')
study.optimize(func=catboost_objective, n_trials=50)

print(f'Best Params : {study.best_params}')
print(f'Best Scores : {study.best_value}')


catboost_best_params = study.best_params

cat = catboost.CatBoostRegressor(
    iterations=catboost_best_params['iterations'],
    learning_rate=catboost_best_params['learning_rate'],
    depth=catboost_best_params['depth'],
    l2_leaf_reg=catboost_best_params['l2_leaf_reg'],
    subsample=catboost_best_params['subsample'],
    colsample_bylevel=catboost_best_params['colsample_bylevel'],
    min_data_in_leaf=catboost_best_params['min_data_in_leaf'],
    max_bin=catboost_best_params['max_bin'],
    eval_metric='RMSE',
    random_state=12,
    verbose=0
)

cat.fit(x_train, y_train)

"""##LightGBM"""

# FUNCTION TO FIND BEST HYPERPARAMETER
# def lgbm_objective(trial):

    # SET HYPERPARAMETERS VALUE RANGE
    # num_leaves        = trial.suggest_int('num_leaves',20,100)
    # max_depth         = trial.suggest_int('max_depth', 2, 15)
    # learning_rate     = trial.suggest_float('learning_rate', 0.001, 0.8)
    # n_estimators      = trial.suggest_int('n_estimators', 100, 2000)
    # min_child_weight  = trial.suggest_float('min_child_weight',1e-3, 1.0)
    # min_child_samples = trial.suggest_int('min_child_samples', 7, 20)
    # subsample         = trial.suggest_float('subsample', 0.4, 1)
    # reg_alpha         = trial.suggest_float('reg_alpha', 0.02, 15)
    # reg_lambda        = trial.suggest_float('reg_lambda', 0.02, 15)

    # # DECLARATE LGBM MODEL
    # lgbm = lightgbm.LGBMRegressor(boosting_type    ='gbdt',
    #                          num_leaves       = num_leaves,
    #                          max_depth        = max_depth,
    #                          learning_rate    = learning_rate,
    #                          n_estimators     = n_estimators,
    #                          min_child_weight = min_child_weight,
    #                          min_child_samples= min_child_samples,
    #                          subsample        = subsample,
    #                          reg_alpha        = reg_alpha,
    #                          reg_lambda       = reg_lambda)

    # METRICS EVALUATION
    # score = cross_val_score(estimator= lgbm, X= x_train, y= y_train, scoring='neg_root_mean_squared_error')

    # return score.mean()

# BUILD AND FIT OPTUNA
# study = optuna.create_study(direction='maximize')
# study.optimize(func= lgbm_objective, n_trials=200)

# print(f'Best Params : {study.best_params}')
# print(f'Best Scores : {study.best_value}')

# #BEST PARAMS
# lgbm_best_params = study.best_params

"""#Creating A Stacking Regressor"""

cv_fold = KFold(n_splits=10, shuffle=True, random_state=12)

model = StackingRegressor(
    estimators=[
        ('Ridge', ridge),
        ('Lasso', lasso),
        ('LinearRegression', linear),       # USING LINEAR REGRESSION CAUSE A MODEL GET WORSE
        ('GradientBoostingRegressor', gbr),
        ('xgb', xgboost),
        #('lightgbm', lgbm),
        ('catboost', cat)
    ],
    cv = cv_fold
)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
y_pred = np.expm1(y_pred)

submission = pd.read_csv('/kaggle/input/home-data-for-ml-course/sample_submission.csv')   # LOAD SAMPLE SUBMISSION CSV

output = pd.DataFrame(y_pred, columns=['SalePrice'])
output = pd.concat([submission.iloc[:,0] , output], axis=1)
output.rename(columns={output.columns[0] : 'Id'}, inplace=True)

# SAVE SUBMISSION
output.to_csv('submission.csv', index=False)